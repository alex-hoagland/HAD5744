plot(marginaleffects(m2)) # Convenience plot
plot_slopes(marginaleffects(m2)) # Convenience plot
?plot_slopes
knitr::opts_chunk$set(echo = TRUE)
################################################################################
# I like to include several additional notes in the header of my files here:
#
# Last modified: 8/12/2022
#
### PURPOSE:
# Lectures 7 and 8 code and output file
#
### NOTES:
# - uses the Tidyverse package and Dplyr
################################################################################
### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(readxl) # Read in data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(AER) # this package has lots of applied metrics packages
library(foreign) # Helpful for reading in data from Stata or other code languages
library(devtools)
library(marginaleffects) # To calculate marginal effects
library(pscl) # For hurdle models
library(nnet) # For multinomial logit models
library(knitr) # Alternative table package
library(kableExtra) # Alternative table package
library(gtsummary) # Alternative table package
set.seed(03262020)
# Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
mydata <- FFTrees::blood # data on blood donation
mydata <- mydata %>% rename(total_donated = total)
mydata %>% summary()
# Main (binary) outcome: did they donate in March 2007
m1 <- lm(donation.crit ~ recency + frequency + time, data=mydata)
# how to interpret the recency dummy?
msummary(list("LPM"=m1),
vcov=c("robust"),
stars=c('*' = .1, '**' = .05, '***' = .01))
# Effect sizes in context?
# Note: each month of recency is associated with a 1 percentage point decline in the likelihood of donating. How relevant is that?
mydata %>% select(donation.crit) %>% summarize(mean = mean(donation.crit)) %>%
mutate(percentage_change = -.01/mean * 100) # What does this mean in context?
# Alternate example: Insurance coverage as a function of enrollee age, sex, BMI, smoker status, and the number of children in the home. Let's estimate and interpret the LPM.
# mydata <- readxl::read_xlsx(here("HealthExpenses.xlsx"))
#
# # Main (binary) outcome: who has coverage?
# m1 <- lm(coverage ~ age + sex + bmi + children + smoker,data=mydata)
# msummary(list("LPM"=m1),
#          vcov=c("robust"),
#          stars=c('*' = .1, '**' = .05, '***' = .01))
?FFTrees
# Logit
library(fixest)
m2 <- feglm(donation.crit ~ recency + frequency + time, data=mydata,
family='binomial'(link='logit')) # since we have binary data
# Probit
m3 <- feglm(donation.crit ~ recency + frequency + time, data=mydata,
family='binomial'(link='probit')) # since we have binary data
msummary(list("LPM"=m1,"Logit"=m2,"Probit"=m3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# Fitted regression "lines"
library(ggdist) # Cool extensions for ggplot
ggplot(mydata,aes(x=recency,y=donation.crit))+
geom_dots(aes(side=ifelse(donation.crit == 1, "bottom", "top")),
pch=19, color="gray70", scale=0.2) +
geom_smooth(method="glm", method.args = list(family = binomial(link = "logit")),
size=1, color='green', se = FALSE) +
labs(x="Months since last donation", y="Pr(Donates)")
# Another option: coefplot (useful in many settings! )
library(fixest) # note: do not use the coefplot library! Much harder to use!
fixest::coefplot(m1,drop = "Constant") + theme_classic() # coefplot of LPM. Why do we like this?
# Now, let's do coefplot of logit
fixest::coefplot(m2,drop = "Constant") + theme_classic()
# See below for coefplot once converted to marginal effects
# 1. Each individual marginal effect
allmarginaleffects <- marginaleffects(m2) # Gives you individual ME for all variables/observations
allmarginaleffects <- allmarginaleffects %>%
mutate(bin = ifelse(p.value > 0.1, 0,
ifelse(p.value <= 0.1 & p.value > 0.05, 1,
ifelse(p.value <= 0.05 & p.value > 0.01, 2, 3))))
allmarginaleffects %>% filter(term == "recency") %>%
ggplot(aes(x=estimate)) +
geom_histogram(alpha=0.4, fill='blue', color='blue') +
theme_classic() +
labs(x="Marginal Effect", y = "Value", fill = "Significance")
# 3. AME
summary(marginaleffects(m2)) # This gives you AME
# 2. and 4. MER and MEM: use the datagrid() function
summary(marginaleffects(m2, datagrid(recency = 7, frequency = 4, time = 31, grid.type = 'counterfactual')))
knitr::opts_chunk$set(echo = TRUE)
n <- 100000
X <- rbinom(n, 1, 0.5)
Z <- rnorm(n, 0, 1)
cor(x,z)
n <- 100000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1)
cor(z, x)
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
# Model 1: unadjusted
sim_1 <- feglm(y ~ x, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Another option: coefplot (useful in many settings! )
library(fixest) # note: do not use the coefplot library! Much harder to use!
# Model 1: unadjusted
sim_1 <- feglm(y ~ x, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2),
vcov=c(rep("robust",2)),
stars=c('*' = .1, '**' = .05, '***' = .01))
### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(readxl) # Read in data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(AER) # this package has lots of applied metrics packages
library(foreign) # Helpful for reading in data from Stata or other code languages
library(devtools)
library(marginaleffects) # To calculate marginal effects
library(pscl) # For hurdle models
library(nnet) # For multinomial logit models
library(knitr) # Alternative table package
library(kableExtra) # Alternative table package
library(fixest)
library(gtsummary) # Alternative table package
set.seed(03262020)
# Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
n <- 100000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
# Model 1: unadjusted
sim_1 <- feglm(y ~ x, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2),
vcov=c(rep("robust",2)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients statistically different?
library(lmtest)
coeftest(sim_1, vcov = sandwich::vcovHC(sim_1, type = "HC1"))
coeftest(sim_2, vcov = sandwich::vcovHC(sim_2, type = "HC1"))
coeftest(sim_1) # unadjusted
coeftest(sim_2) # adjusted
# are the coefficients on x statistically different across sim_1 and sim_2?
library(systemfit)
library(car)
# Two nested models
eq1 <- y ~ x
eq2 <- y ~ x + z
?systemfit
b1 <- coef(sim_1)["x"]
b2 <- coef(sim_2)["x"]
se1 <- vcov(sim_1)["x", "x"]
se2 <- vcov(sim_2)["x", "x"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval
n <- 10000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
# Model 1: unadjusted
sim_1 <- feglm(y ~ x, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2),
vcov=c(rep("robust",2)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x"]
b2 <- coef(sim_2)["x"]
se1 <- vcov(sim_1)["x", "x"]
se2 <- vcov(sim_2)["x", "x"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval
n <- 20000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
simdata <- simdata %>% mutate(y1 = ifelse(row_number() <= 10000, y, NA),
x1 = ifelse(row_number() <= 10000, x, NA),
z1 = ifelse(row_number() <= 10000, z, NA))
# Model 1: unadjusted
sim_1 <- feglm(y1 ~ x1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y1 ~ x1 + z1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 3: add underlying data
sim_3 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2,"Adjusted, More Data"=sim_3),
vcov=c(rep("robust",2)),
stars=c('*' = .1, '**' = .05, '***' = .01))
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2,"Adjusted, More Data"=sim_3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x"]
b2 <- coef(sim_2)["x"]
se1 <- vcov(sim_1)["x", "x"]
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x1"]
b2 <- coef(sim_2)["x1"]
se1 <- vcov(sim_1)["x1", "x1"]
se2 <- vcov(sim_2)["x1", "x1"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval # not different here
# But what about the odds ratios?
b3 <- coef(sim_3)["x"]
or1 <- exp(b1)
or2 <- exp(b2)
or3 <- exp(b3)
c(or1,or2,or3)
n <- 200000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
simdata <- simdata %>% mutate(y1 = ifelse(row_number() <= 100000, y, NA),
x1 = ifelse(row_number() <= 100000, x, NA),
z1 = ifelse(row_number() <= 100000, z, NA))
# Model 1: unadjusted
sim_1 <- feglm(y1 ~ x1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y1 ~ x1 + z1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 3: add underlying data
sim_3 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2,"Adjusted, More Data"=sim_3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x1"]
b2 <- coef(sim_2)["x1"]
se1 <- vcov(sim_1)["x1", "x1"]
se2 <- vcov(sim_2)["x1", "x1"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval # not different here
n <- 1000000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
simdata <- simdata %>% mutate(y1 = ifelse(row_number() <= 500000, y, NA),
x1 = ifelse(row_number() <= 500000, x, NA),
z1 = ifelse(row_number() <= 500000, z, NA))
# Model 1: unadjusted
sim_1 <- feglm(y1 ~ x1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y1 ~ x1 + z1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 3: add underlying data
sim_3 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2,"Adjusted, More Data"=sim_3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x1"]
b2 <- coef(sim_2)["x1"]
se1 <- vcov(sim_1)["x1", "x1"]
se2 <- vcov(sim_2)["x1", "x1"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval # not different here
b3 <- coef(sim_3)["x"]
or1 <- exp(b1)
or2 <- exp(b2)
or3 <- exp(b3)
c(or1,or2,or3)
# Do the marginal effects in each model change?
me1 <- summary(marginaleffects(sim_1))$AME[1]
# Do the marginal effects in each model change?
me1 <- summary(slopes(sim_1))$AME[1]
?marginaleffets
?marginaleffects
library(margins) # To calculate marginal effects
install.packages('margins')
library(margins) # To calculate marginal effects
margins(sim_1)
marginaleffects::predictions(sim_1, newdata=datagrid(x=1,z=1))
marginaleffects::predictions(sim_1, newdata=datagrid(x1=1,z1=1))
marginaleffects::predictions(sim_1, newdata=datagrid(x1=mean(simdata$x1),z1=1))
marginaleffects::predictions(sim_1, newdata=datagrid(x1=mean(simdata$x1),z1=mean(simdata$z1)))
marginaleffects::predictions(sim_1)
mean(marginaleffects::predictions(sim_1))
mean(marginaleffects::predictions(sim_1), na.rm=T)
marginaleffects::predictions(sim_1)
mean(marginaleffects::predictions(sim_1)$Estimate)
?avg_predictinos
?avg_predictions
avg_predictions(sim_1)
# Do the marginal effects in each model change?
avg_predictions(sim_1)
avg_predictions(sim_2)
avg_predictions(sim_3)
me1 <- avg_predictions(sim_1)
me2 <- avg_predictions(sim_2)
me3 <- avg_predictions(sim_3)
c(me1,me2,me3)
# Do the marginal effects in each model change?
me1 <- avg_predictions(sim_1)$estimate
me2 <- avg_predictions(sim_2)$estimate
me2 <- avg_predictions(sim_2)$estimate
me3 <- avg_predictions(sim_3)$estimate
c(me1,me2,me3)
plot_predictions(sim_2, by="x1")
# Do the marginal effects in each model change?
me1 <- avg_comparisons(sim_1)$estimate
avg_comparisons(sim_1)
avg_comparisons(sim_2)
avg_comparisons(sim_2)$estimate
# Do the marginal effects in each model change?
me1 <- avg_comparisons(sim_1)$estimate
me2 <- avg_comparisons(sim_2)$estimate[1]
me3 <- avg_comparisons(sim_3)$estimate[1]
c(me1,me2,me3)
c(or1,or2,or3)
### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(readxl) # Read in data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(AER) # this package has lots of applied metrics packages
library(foreign) # Helpful for reading in data from Stata or other code languages
library(devtools)
library(marginaleffects) # To calculate marginal effects
library(pscl) # For hurdle models
library(nnet) # For multinomial logit models
library(knitr) # Alternative table package
library(kableExtra) # Alternative table package
library(fixest)
library(gtsummary) # Alternative table package
set.seed(03262020)
# Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
mydata <- FFTrees::blood # data on blood donation
mydata <- mydata %>% rename(total_donated = total)
mydata %>% summary()
# Main (binary) outcome: did they donate in March 2007
m1 <- lm(donation.crit ~ recency + frequency + time, data=mydata)
# how to interpret the recency dummy?
msummary(list("LPM"=m1),
vcov=c("robust"),
stars=c('*' = .1, '**' = .05, '***' = .01))
# Effect sizes in context?
# Note: each month of recency is associated with a 1 percentage point decline in the likelihood of donating. How relevant is that?
mydata %>% select(donation.crit) %>% summarize(mean = mean(donation.crit)) %>%
mutate(percentage_change = -.01/mean * 100) # What does this mean in context?
# Alternate example: Insurance coverage as a function of enrollee age, sex, BMI, smoker status, and the number of children in the home. Let's estimate and interpret the LPM.
# mydata <- readxl::read_xlsx(here("HealthExpenses.xlsx"))
#
# # Main (binary) outcome: who has coverage?
# m1 <- lm(coverage ~ age + sex + bmi + children + smoker,data=mydata)
# msummary(list("LPM"=m1),
#          vcov=c("robust"),
#          stars=c('*' = .1, '**' = .05, '***' = .01))
# Look at predictions
mydata$pred_donation <- predict(m1, mydata)
hist(mydata$pred_donation) # what problems do you see here?
summary(mydata) # First problem: some predictions are outside of the unit interval!
mydata %>% filter(pred_donation > 1.1) # What does it mean to have a 110% probability of donating?
hist(mydata$pred_donation) # Second problem: how do you go from a predicted 57% probability of Y=1 to predicting Y?
mydata <- mydata %>% mutate(yhat = ifelse(pred_donation >= 0.5, 1, 0)) # What happens if we assume a cutoff (Yhat >= 50% means Yhat = 1)? How good is our match then?
table(mydata$donation.crit, mydata$yhat) # We incorrectly match 22.6% of the data! And we only predict a few donations. Can we play around with this (lower thresholds?)
# Logit
library(fixest)
m2 <- feglm(donation.crit ~ recency + frequency + time, data=mydata,
family='binomial'(link='logit')) # since we have binary data
# Probit
m3 <- feglm(donation.crit ~ recency + frequency + time, data=mydata,
family='binomial'(link='probit')) # since we have binary data
msummary(list("LPM"=m1,"Logit"=m2,"Probit"=m3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# Fitted regression "lines"
library(ggdist) # Cool extensions for ggplot
ggplot(mydata,aes(x=recency,y=donation.crit))+
geom_dots(aes(side=ifelse(donation.crit == 1, "bottom", "top")),
pch=19, color="gray70", scale=0.2) +
geom_smooth(method="glm", method.args = list(family = binomial(link = "logit")),
size=1, color='green', se = FALSE) +
labs(x="Months since last donation", y="Pr(Donates)")
# Another option: coefplot (useful in many settings! )
library(fixest) # note: do not use the coefplot library! Much harder to use!
fixest::coefplot(m1,drop = "Constant") + theme_classic() # coefplot of LPM. Why do we like this?
# Now, let's do coefplot of logit
fixest::coefplot(m2,drop = "Constant") + theme_classic()
# See below for coefplot once converted to marginal effects
n <- 1000000
x <- rbinom(n, 1, 0.5)
z <- rnorm(n, 0, 1) # note thta x and z are uncorrelate by assumption
# True model: logit(P(Y=1)) = -2 + 1*X + 0.5*Z
linpred <- -2 + 1*x + 0.5*z
p <- 1 / (1 + exp(-linpred))
y <- rbinom(n, 1, p)
simdata <- data.frame(y, x, z)
simdata <- simdata %>% mutate(y1 = ifelse(row_number() <= 500000, y, NA),
x1 = ifelse(row_number() <= 500000, x, NA),
z1 = ifelse(row_number() <= 500000, z, NA))
# Model 1: unadjusted
sim_1 <- feglm(y1 ~ x1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 2: adjutsed for Z
sim_2 <- feglm(y1 ~ x1 + z1, data=simdata,
family='binomial'(link='logit')) # since we have binary data
# Model 3: add underlying data
sim_3 <- feglm(y ~ x + z, data=simdata,
family='binomial'(link='logit')) # since we have binary data
msummary(list("Unadjsuted"=sim_1,"Adjusted"=sim_2,"Adjusted, More Data"=sim_3),
vcov=c(rep("robust",3)),
stars=c('*' = .1, '**' = .05, '***' = .01))
# are the coefficients on x statistically different across sim_1 and sim_2?
b1 <- coef(sim_1)["x1"]
b2 <- coef(sim_2)["x1"]
se1 <- vcov(sim_1)["x1", "x1"]
se2 <- vcov(sim_2)["x1", "x1"]
# Approximate z-test for equality of coefficients (assuming independence)
z <- (b1 - b2) / sqrt(se1 + se2)
pval <- 2 * pnorm(-abs(z))
pval # coefficient has totally changed!
# So then the odds ratios change as well?
b3 <- coef(sim_3)["x"]
or1 <- exp(b1)
or2 <- exp(b2)
or3 <- exp(b3)
c(or1,or2,or3)
# Do the marginal effects in each model change?
me1 <- avg_comparisons(sim_1)$estimate
me2 <- avg_comparisons(sim_2)$estimate[1]
me3 <- avg_comparisons(sim_3)$estimate[1]
c(me1,me2,me3)
# 1. Each individual marginal effect
allmarginaleffects <- predictions(m2) # Gives you individual ME for all variables/observations
View(allmarginaleffects)
# 1. Each individual marginal effect
allmarginaleffects <- comparisons(m2) # Gives you individual ME for all variables/observations
View(allmarginaleffects)
allmarginaleffects <- allmarginaleffects %>%
mutate(bin = ifelse(p.value > 0.1, 0,
ifelse(p.value <= 0.1 & p.value > 0.05, 1,
ifelse(p.value <= 0.05 & p.value > 0.01, 2, 3))))
allmarginaleffects %>% filter(term == "recency") %>%
ggplot(aes(x=estimate)) +
geom_histogram(alpha=0.4, fill='blue', color='blue') +
theme_classic() +
labs(x="Marginal Effect", y = "Value", fill = "Significance")
# 2. and 4. MER and MEM: use the datagrid() function
summary(comparisons(m2, datagrid(recency = 7, frequency = 4, time = 31, grid.type = 'counterfactual')))
# 2. and 4. MER and MEM: use the datagrid() function
summary(comparisons(m2, newdata=datagrid(recency = 7, frequency = 4, time = 31, grid.type = 'counterfactual')))
# 2. and 4. MER and MEM: use the datagrid() function
summary(comparisons(m2, newdata=datagrid(recency = 7, frequency = 4, time = 31)))
# 2. and 4. MER and MEM: use the datagrid() function
summary(avg_comparisons(m2, newdata=datagrid(recency = 7, frequency = 4, time = 31)))
# 2. and 4. MER and MEM: use the datagrid() function
avg_comparisons(m2, newdata=datagrid(recency = 7, frequency = 4, time = 31))
# 3. AME
avg_comparisons(m2) # This gives you AME
