---
title: "Lecture 3 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r header}
################################################################################
# I like to include several additional notes in the header of my files here:
#
# Last modified: 8/11/2022
#
### PURPOSE:
  # Lecture 3 code and output file
# 
### NOTES:
  # - uses the Tidyverse package and Dplyr
################################################################################


### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(faux) # Useful package for simulating data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(dotwhisker) # For coefficient plots
library(gapminder) # For coefficient plots

set.seed(122333) # Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
```

## Control Variables

First, let's explore the relationship between A1C checks and hospitalizations. We'll use some fake data for this, so don't trust any of the results you see reported here! 

```{r fake-data,echo=F}
# Create a data set of 1000 physicians, with some rates of hospitalizations, A1Cs, and average patient education (in years)

covmat <- matrix(c(1,.5,-.2,.5,1,.5,-.2,.5,1),nrow=3,ncol=3) # Covariance matrix used to simulate the data
mydata <- rnorm_multi(1000,3,0,1,covmat,varnames=c("hospitalizations","a1cs","education"))
view(mydata) # this command lets us look at the data (but we don't want it to print out into the rmd, hence the echo=F)
```

### How do controls change the results?

If we don't include controls, we get a simple regression of just the effect of A1Cs on hospitalizations: 
```{r simple}
lm_simple <- lm(hospitalizations ~ a1cs, data=mydata)
summary(lm_simple)
```

But if we control for average patient education, the results change: 

```{r control-education}
# Now, control for education 
lm_complex <- lm(hospitalizations ~ a1cs + education, data=mydata)
summary(lm_complex)
```
How can we visualize these regressions? Let's use a coefficient plot! 
```{r coefplot}
dwplot(list(lm_simple,lm_complex)) # how can you customize this? 
# a tutorial: https://felixhaass.github.io/dataviz_ggplot2/session4.html

# To customize, let's combine our models as objects
m1_tidy <- tidy(lm_simple) %>% 
  mutate(model = "Baseline")

# repeat for model 2
m2_tidy <- tidy(lm_complex) %>% 
  mutate(model = "Additional Controls")

# "glue" model data frames together
all_models <- bind_rows(m1_tidy, m2_tidy) %>% 
  filter(term != "(Intercept)") %>% # remove the intercept
  relabel_predictors(c(a1cs = "Rate of A1C checks", 
                       education = "Average Education"))

# An example of customization (compare with slides)
dwplot(all_models, 
       # here are our regular aesthetics
       dot_args = list(aes(colour = model, 
                       shape = model, size=1.2)), 
       whisker_args = list(size=1)) + 
  theme_bw() +
  labs(title = "Predicting Diabetic Hospitalization Rates", 
       x = "Coefficient Estimate with 95% CIs", 
       y = "", size="",
       caption = "Note: All independent variables are measured as % relative to baseline mean.") +
  geom_vline(xintercept = 0, linetype = "dashed", size=1) +
  theme(plot.title = element_text(face="bold"),
        legend.position = "bottom",
        legend.background = element_rect(colour="grey80"),
        legend.title.align = .5) +
  guides(size = "none") + # remove the size legend
  scale_shape_discrete(name  ="Models", breaks = c(0, 1)) + # breaks assign shapes
  scale_color_discrete(type=c("#E69F00", "#56B4E9"),name = "Models") # breaks assign colors
# ggsave(here("coefplot.png"), width = 8, height = 6, dpi = 300) # save the plot
```

## Inference in regression 

### How to obtain standard errors
Just like a lot of other things, standard errors are stored in the lm object! 

```{r ses}
myout <- summary(lm_complex)
myout$coefficients[,2]
```

### Hypothesis testing

Let's start with a population model where: 
$$ y = 3 + .2x + \varepsilon, \varepsilon \sim \mathcal{N}(0,1).$$

We will use this DGP to create a small data set ($N=200$) and run a simple regression. Then, we will save the coefficient $\beta_1$ and its standard error. 
```{r hyp-test}
set.seed(0326)
nulldata <- tibble(x=rnorm(200),
                 epsilon=rnorm(200),
                 y=3+.2*x+epsilon)
m1 <- lm(y ~ x,data=nulldata)
out <- summary(m1) # I get a first estimate of beta = 0.12, and a SE = 0.064
myse <- out$coefficients[2,2] # store the SE
```

Our test, formally, is of the null hypothesis: 

$$ \mathcal{H}_0: \beta_1 = 0. $$
Let's plot our regression estimate against the sampling distribution of the null hypothesis
```{r hyp-test-2} 
myalf <- .05 # State the rejection probability

# Plot my estimate against the sampling distribution of the null hypothesis 
graphdata <- tibble(x=rnorm(1e4,mean=0,sd=myse), # normal distribution
                    xrej = ifelse(x >= qnorm(1-(myalf)/2, mean=0, sd=myse),x,NA), # x's that fall in the rejection region based on alpha
                    xrej2 = ifelse(-x >= qnorm(1-(myalf)/2, mean=0, sd=myse),x,NA), # x's that fall in the rejection region based on alpha
                    y=dnorm(x,mean=0,sd=myse)
)

 ggplot(data=graphdata) + 
  geom_density(aes(x)) + 
  geom_area(data=subset(graphdata, !is.na(xrej)),aes(x=x,y=y),fill="gray") + 
   geom_area(data=subset(graphdata, !is.na(xrej2)),aes(x=x,y=y),fill="gray") + 
  geom_vline(xintercept=0.12,linetype='dashed',color='red') + 
  theme_minimal() + 
  labs(x="Estimated Beta",y="Density")
```

So what do we conclude from the test? 

#### One-sided testing

What happens if I perform a one-sided test? This changes the rejection region (is that disingenious?)
```{r hyp-test-3} 
graphdata <- tibble(x=rnorm(1e5,mean=0,sd=myse), # normal distribution
                     xrej = ifelse(x >= qnorm(1-(myalf), mean=0, sd=myse),x,NA), 
                        # x's that fall in the rejection region based on alpha
                     y=dnorm(x,mean=0,sd=myse)
 )
 
 ggplot(data=graphdata) + 
   geom_density(aes(x)) + 
   geom_area(data=subset(graphdata, !is.na(xrej)),aes(x=x,y=y),fill="gray") + 
   geom_vline(xintercept=0.12,linetype='dashed',color='red') + 
   theme_minimal() + 
   labs(x="Estimated Beta",y="Density")
```

#### Statistical power

In general, adding more data adds to our statistical power -- the probability that we will uncover an effect that is truly in the DGP. Let's increase our sample size to 20,000 and see how this affects our standard error (and hence, our test statistic)

```{r hyp-test-4}
 nulldata <- tibble(x=rnorm(20000),
                  epsilon=rnorm(20000),
                  y=3+.2*x+epsilon)
 m1 <- lm(y ~ x,data=nulldata)
 out <- summary(m1) # 
 out
 myse <- out$coefficients[2,2] # store the SE
 
 graphdata <- tibble(x=rnorm(1e5,mean=0,sd=myse), # normal distribution
                     xrej = ifelse(x >= qnorm(1-(myalf)/2, mean=0, sd=myse),x,NA), 
                        # x's that fall in the rejection region based on alpha
                     xrej2 = ifelse(-x >= qnorm(1-(myalf)/2, mean=0, sd=myse),x,NA), 
                        # x's that fall in the rejection region based on alpha
                     y=dnorm(x,mean=0,sd=myse)
 )
 
 ggplot(data=graphdata) + 
   geom_density(aes(x)) + 
   geom_area(data=subset(graphdata, !is.na(xrej)),aes(x=x,y=y),fill="gray") + 
   geom_area(data=subset(graphdata, !is.na(xrej2)),aes(x=x,y=y),fill="gray") + 
   geom_vline(xintercept=0.12,linetype='dashed',color='red') + 
   theme_minimal() + 
   labs(x="Estimated Beta",y="Density")
 
# Calculate p-value of our estimated beta
1-pnorm(out$coefficients[2,1],mean=0,sd=out$coefficients[2,2])

# Calculate the t-statistic of our estimated beta
out$coefficients[2,1]/out$coefficients[2,2] # Here, critical value is 1.96
```

## Dummy Variables

Let's add another control in the form of a dummy variable: physician sex (we want this to be binary for the purposes of the regression, so we will use sex instead of gender identity).

```{r unorganized}
mydata <- mydata %>% mutate(female = ifelse(runif(nrow(mydata))>0.6, 1, 0),
                            male = 1-female)

# What is the gender composition of our sample?
summary(mydata$female)
```

Now let's include this dummy variable in the regression, and compare all three results in a regression table using modelsummary 

```{r dummy-reg}
# Linear model with dummy variable included
lm_dummy <- lm(hospitalizations ~ a1cs + education + female, data=mydata)
summary(lm_dummy) # What does it mean that this is significant? Should it be? 
  # Return to this at the end of the lecture when we talk about validity traps

# Let's make a regression table for ease of interpretation
msummary(list("Simple"=lm_simple,"One Control"=lm_complex,"Full"=lm_dummy),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

### Dummy Variable Trap

What happens if we include both $1\{male\}$ and $1\{female\}$ in a regression? 

```{r dvt}
lm_dummytrap <- lm(hospitalizations ~ a1cs + education + female + male, data=mydata)
summary(lm_dummytrap)
  # Code can usually detect the dummy variable trap, but don't count on it! 
```

### Multiple dummy variables

Finally, how does this change if we have multiple dummy variables in a set? Let's consider the region in which each physician practices. 

```{r region} 
mydata <- mydata %>% mutate(region = sample(seq(1:4),size=nrow(mydata),replace=TRUE)) 
  # Generate a random region variable for restaurants 

# Suppose that 1=West, 2=Midwest, 3=South, 4=East
# Need to generate three dummy variables for the regression 
mydata <- mydata %>% mutate(region_west = (region == 1), 
                      region_midwest = (region == 2), 
                      region_south = (region == 3))
lm_region <- lm(hospitalizations ~ a1cs + education + female + region_west + region_midwest + region_south, data=mydata)
msummary(list("No Regions"=lm_dummy, "With Region Controls"=lm_region),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
  # Why aren't our other coefficients changed? Thoughts? 

# # Joint test of significance -- helpful to consider after lecture
# library('lmtest')
# lmtest::waldtest(lm_dummy, lm_region) # Run a version of the regression without the variables you want to test, 
# # and one with full set of coefficients. Then use waldtest to test difference
```

## Making a Regression Table

### Preliminaries 

When making and saving outputs, you want your code (and outputs) to be **completely replicable**. You will absolutely have to make these figures and tables more times than you think you should reasonably have to! 

So make sure you have: 
1. An R project set up
2. A command that points R to a directory **regardless of your machine** 
    a. This is where the "here" package comes in handy
3. Consistent project organization (folders) with dates for each replication of a figure

```{r reg-tables}
# Make sure your directory is working
here() # Note: this will be the directory your project is stored in

# Some sample data for our table construction
res <- causaldata::restaurant_inspections

res <- res %>%
  # Create NumberofLocations
  group_by(business_name) %>%
  mutate(NumberofLocations = n())
summary(res$NumberofLocations)

## Let's run our regression models
m1 <- lm(inspection_score ~ NumberofLocations, data = res)
m2 <- lm(inspection_score ~ NumberofLocations + Year + Weekend, data = res)

# Give msummary a list() of the models we want in our table
# and save to a file using the here() library
# Note that you select the file type (html, pdf, etc.)
# (see help(msummary) for other options)
gm <- tibble::tribble(
  ~raw,        ~clean,          ~fmt,
  "nobs",      "N",             0)

msummary(list("Simple"=m1,"Full"=m2), # Rename columns
         stars=TRUE, # Include stars for statistical significance
         fmt = 2, # how many decimal places do you want? 
        coef_rename = c("(Intercept)" = "Intercept", 
                        "NumberofLocations" = "Number of Locations", 
                        "Year" = "Year", 
                        "WeekendTRUE" = "Inspection on a Weekend"), # Rename the rows
        gof_omit = 'DF|Deviance|RMSE|AIC|BIC|Log.Lik', # Omit some of the goodness of fit stats
        gof_map = gm) # If you just want to include N here (how could you add in, say, R2?)
         # output= here('regression_table.html')) 
# If you want a folder within your directory, you would say here("Output", "RegressionTable.html")

# Default significance stars are +/*/**/*** .1/.05/.01/.001. Social science
# standard */**/*** .1/.05/.01 can be restored with stars=c('*' = .1, '**' = .05, '***' = .01)
```

## Package Citations
```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS. 
```