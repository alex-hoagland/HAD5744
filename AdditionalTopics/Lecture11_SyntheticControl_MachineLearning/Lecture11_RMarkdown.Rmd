---
title: "Lecture 10 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preliminaries}
################################################################################
# I like to include several additional notes in the header of my files here:
#
# Last modified: 8/15/2022
#
### PURPOSE:
  # Lecture 11 code and output file
  # synthetic control + quantile regression
# 
### NOTES:
  # - uses the Tidyverse package and Dplyr
################################################################################


### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(readxl) # Read in data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(AER) # this package has lots of applied metrics packages
library(foreign) # Helpful for reading in data from Stata or other code languages 
library(zoo) # Helpful packages for organizing dates
library(tidysynth) # For synthetic controls
library(gsynth) # For synthetic controls
library(gghighlight) # For figures 
library(lubridate) # For figures
library(stargazer) # For tables
library(quantreg) # For quantile regression
library(binsreg) # For binscatters
library(nprobust) # Local linear regression

set.seed(03262020) 
  # Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################
```

## Synthetic Controls

We'll continue our question from last time -- what is the effect of a vaccine lottery on COVID-19 vaccination rates? In this case, we'll replicate a paper by David Lang, Lief Esbenshade, and Robb Willer (paper available [here](https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/did-ohios-vaccine-lottery-increase-vaccination-rates-a-preregistered-synthetic-control-study/07720E0BB974962FE4547FF2BCC71CAC#supplementary-materials).). A full replication code and dataset are available in the Github repo (you can also directly download the data [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QYXN9L)). 

### Preliminaries

Let's load the replication data available from the authors and make some descriptive figures. 
```{r load-data}
mydata <- readRDS(here("weekly_data_2021-06-24.rds")) 

# Some descriptive figures
ggplot(mydata, aes(x = last_day, 
                y = people_fully_vaccinated_per_hundred, 
                group = state)) +
  geom_line() + 
  gghighlight(state=="OH",
              label_params = list(fill = NA, alpha=1)) +
  geom_vline(xintercept = lubridate::make_date(2021, 5, 12), linetype = "solid") +
  labs(
    title = "Vaccination Rates by State by Week",
    caption = "Timing of The Ohio Lottery Announcement",
    x = "Date",
    y = "Percent Fully Vaccinated"
  ) +
  theme_minimal()

# Ranked Vaccination Data
ranks <- mydata %>% 
  filter(centered_week == 4)  %>% 
  arrange(desc(people_fully_vaccinated_per_hundred)) %>% 
  mutate(rank=row_number()) %>% 
  select(state,people_fully_vaccinated_per_hundred,rank) # Ohio is solidly middle of the pack
```

### Constructing the Control 

This code selects the variables used for matching and constructs synthetic Ohio. It also constructs synthetic states for all other states in the specified donor pool, which will be helpful for our placebo tests later; this does mean, however, that it takes a minute or two to run. 

```{r synth-1}
# Construct Synthetic Controls
vaccine_out <-
  mydata  %>%
  # initial the synthetic control object
  synthetic_control(outcome = people_fully_vaccinated_per_hundred, # outcome
                    unit = state, # unit index in the panel data
                    time = centered_week, # time index in the panel data
                    i_unit = "OH", # unit where the intervention occurred
                    i_time = 0, # time period when the intervention occurred
                    generate_placebos=T # generate placebo synthetic controls (for inference)
  ) %>%
  # Matching on fully vaccinated the weeks before the intervention  
  generate_predictor(time_window = -17, lagged_vaccinations_week17 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -16, lagged_vaccinations_week16 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -15, lagged_vaccinations_week15 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -14, lagged_vaccinations_week14 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -13, lagged_vaccinations_week13 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -12, lagged_vaccinations_week12 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -11, lagged_vaccinations_week11 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -10, lagged_vaccinations_week10 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -09, lagged_vaccinations_week09 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -08, lagged_vaccinations_week08 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -07, lagged_vaccinations_week07 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -06, lagged_vaccinations_week06 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -05, lagged_vaccinations_week05 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -04, lagged_vaccinations_week04 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -03, lagged_vaccinations_week03 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -02, lagged_vaccinations_week02 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -01, lagged_vaccinations_week01 = people_fully_vaccinated_per_hundred) %>%
  # Generate the fitted weights for the synthetic control
  generate_weights(optimization_window = -17:-1, # time to use in the optimization task
                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options
  ) %>%
  # Generate the synthetic control
  generate_control()
```

### Evaluating the Control

Let's explore what the synthetic control came up with -- specifically, what states make up synthetic Ohio? What variables are we matching on? How good is the balance between Ohio and synthetic Ohio?

```{r synth-eval}
# Which states are we using, and what weights are they given? 
vaccine_out %>%
  grab_unit_weights() %>%
  mutate(weights = round(weight, digits = 4)) %>%
  select(unit, weights) %>%
  filter(weights>0.0001) %>%
  as.data.frame() %>%
  stargazer(summary = FALSE, rownames = FALSE)

# What about the independent variables?
vaccine_out %>% 
  plot_weights() + 
  labs(title="Synthetic Control Weights")   

# Balance Table
vaccine_out %>%
  grab_balance_table() %>%
  mutate(difference = OH - synthetic_OH) %>%
  select(variable, OH, synthetic_OH, difference, donor_sample) %>%
  as.data.frame() %>%
  stargazer(summary = FALSE, rownames = FALSE, 
            caption = "Balance Table", 
            label = "balancetable", type="html") # Note: try this in R Markdown
```

### Interpreting the Synthetic Control

So what's the effect? To find out, let's compare Ohio and Synthetic Ohio over time: 

```{r synth-plot}
vaccine_out %>% plot_trends() +
  scale_x_continuous(breaks = c(-15,-10,-5,0,5)) +
  labs(
    title = "Ohio and Synthetic Ohio",
    caption = "Timing of The Lottery Announcement",
    x="Weeks Relative to Lottery Announcement",
    y="Percent Fully Vaccinated"
  ) 

# Plot Model Differences
vaccine_out %>% plot_differences() +
  scale_x_continuous(breaks = c(-15,-10,-5,0,5)) +
  labs(
    title = "Difference between Ohio and Synthetic Ohio",
    caption = "Timing of The Lottery Announcement",
    x="Weeks Relative to Lottery Announcement",
    y="Difference in Percent Fully Vaccinated"
  ) 
```

### Inference

How do we get a sense of whether the effect was significant? To do this, let's plot the placebo differences between each state and it's "synthetic state" in the world where the vaccine lottery never took place (hint: the lottery *never* took place in *any* of these states).

```{r synth-placebo}
# Plot placebos of different states' assignments
vaccine_out %>% plot_placebos() +
  scale_x_continuous(breaks = c(-15,-10,-5,0,5)) +
  labs(
    title = "Difference between State and Synthetic State: All States",
    caption = "Timing of The Lottery Announcement",
    x="Weeks Relative to Lottery Announcement",
    y="Difference in Percent Fully Vaccinated"
  ) 
```

We can also run a placebo test across time, rather than geography: 
```{r placebo-time}
# This test shifts the pre-treatment window back five weeks.
# This analysis was included in our pre-registration as a demonstration of the 
# method and to show that we did not find treatment effects before the lottery 
# was announced. 

placebo_out <-
  mydata %>% 
  filter(centered_week <= 0) %>% 
  # initial the synthetic control object
  synthetic_control(outcome = people_fully_vaccinated_per_hundred, # outcome
                    unit = state, # unit index in the panel data
                    time = centered_week, # time index in the panel data
                    i_unit = "OH", # unit where the intervention occurred
                    i_time = -5, # time period when the intervention occurred
                    generate_placebos=T # generate placebo synthetic controls (for inference)
  ) %>%
  # Matching on fully vaccination the weeks before the intervention  
  generate_predictor(time_window = -17, people_fully_vaccinated_per_hundred17 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -16, people_fully_vaccinated_per_hundred16 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -15, people_fully_vaccinated_per_hundred15 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -14, people_fully_vaccinated_per_hundred14 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -13, people_fully_vaccinated_per_hundred13 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -12, people_fully_vaccinated_per_hundred12 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -11, people_fully_vaccinated_per_hundred11 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -10, people_fully_vaccinated_per_hundred10 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -09, people_fully_vaccinated_per_hundred09 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -08, people_fully_vaccinated_per_hundred08 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -07, people_fully_vaccinated_per_hundred07 = people_fully_vaccinated_per_hundred) %>%
  generate_predictor(time_window = -06, people_fully_vaccinated_per_hundred06 = people_fully_vaccinated_per_hundred) %>%
  # Generate the fitted weights for the synthetic control
  generate_weights(optimization_window = -17:-6, # time to use in the optimization task
                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 # optimizer options
  ) %>%
  # Generate the synthetic control
  generate_control()


placebo_out %>% plot_trends()  + 
  labs(
    title = "Placebo Analysis: Ohio and Synthetic Ohio",
    caption = "Timing of The Placebo Announcement",
    x="Weeks Relative to Lottery Announcement",
    y="Percent Fully Vaccinated"
  )


placebo_out %>% plot_differences()  + 
  labs(
    title = "Placebo Analysis:  Difference between Ohio and Synthetic Ohio",
    caption = "Timing of The Placebo Announcement",
    x="Weeks Relative to Lottery Announcement",
    y="Percent Fully Vaccinated"
  )

# This was giving me problems, needed to run in the console 
# placebo_out %>% grab_significance() %>% filter(unit_name == "OH")
placebo_out %>% grab_unit_weights() %>% arrange(desc(weight))

placebo_out %>% plot_mspe_ratio() 
```

## Quantile Regression 

Let's examine the effect of health states on something that has a really wonky distribution: health expenditures. Do we expect something that disrupts one's health state to affect all individuals equally? 

### OLS Regression

Let's zero in on a health event that limits activity. How would this affect health expenditures for those with already high health expenses (and probably, more complex health needs) versus those starting with low expenses (where the activity limitation may impact a patient's ability to receive care). This seems like a potentially large distributional difference we need to care about. 

We'll use data from the [MEPS](https://r.search.yahoo.com/_ylt=AwrCxnYOj_piDR8AQDoXFwx.;_ylu=Y29sbwNiZjEEcG9zAzEEdnRpZAMEc2VjA3Ny/RV=2/RE=1660616591/RO=10/RU=https%3a%2f%2fmeps.ahrq.gov%2fmepsweb%2f/RK=2/RS=gj3n9FRF_YXHQW1ApHdX7BXB.Sc) and start with a basic OLS regression. 

```{r quantile-1}
# New data: health expenditures
mydata <- read.dta(here("heus_mepssample.dta")) # An extract from MEPS
hist(mydata$exp_tot) # Why does this look backwards from the figure we just showed on slides?

# OLS Regression: What is the effect of activity limitation on spending? 
m_ols <- lm(exp_tot ~ anylim + age + female + race_bl + race_oth +eth_hisp + famsize + ed_hs + ed_hsplus + ed_col + lninc + reg_midw + reg_south + reg_west + ins_mcare + ins_mcaid + ins_unins + ins_dent,
            data=mydata,
            weights=wtdper) # Note the use of weights
msummary(list("OLS"=m_ols),
         vcov=c(rep("robust",1)),
         stars=c('*' = .1, '**' = .05, '***' = .01))
# At the mean: activity limitation is associated with $4000 increase in spending
summary(mydata$exp_tot) # How to interpret this? 
plot(m_ols) # QQ plot -- shows that the distribution is highly nonnormal
```

How do we interpret what we've found here? What does the QQ plot tell us? 

### LAD (Median) Regression

So maybe fitting the conditional average isn't telling us much--what can we say about how an activity limitation might affect the **median** spender? How does the median spender differ from the average? 

```{r lad}
m_lad <- rq(exp_tot ~ anylim + age + female + race_bl + race_oth +eth_hisp + famsize + ed_hs + ed_hsplus + ed_col + lninc + reg_midw + reg_south + reg_west + ins_mcare + ins_mcaid + ins_unins + ins_dent,
            data=mydata,
            weights=wtdper,
            tau = 0.5)
msummary(list("OLS"=m_ols,"LAD"=m_lad),
         stars=c('*' = .1, '**' = .05, '***' = .01)) 
```

Now, the effect of activity limitation is estimated to be $1578.93, much lower than before. Does that feel right to us? 

### Quantile Regression

What about across the entire distribution of spending? Now we'll switch to a quantile approach, running a regression across ten deciles of the spending distribution:

```{r quantile-2}
mytau <- rep(NA,10) # empty vector: quantiles
coefs <- rep(NA, 10) # empty vector: coefficients
lb <- rep(NA, 10) # empty vector: 95% LB
ub <- rep(NA, 10) # empty vector: 95% UB
for (t in 1:10) {
  mytau[t] <- t/10 # indicate which decile I am using
  print(paste0("Considering quantile ",mytau[t],sep=" "))
  myreg <- rq(exp_tot ~ anylim + age + female + race_bl + race_oth +eth_hisp + famsize + ed_hs + ed_hsplus + ed_col + lninc + reg_midw + reg_south + reg_west + ins_mcare + ins_mcaid + ins_unins + ins_dent,
              data=mydata,
              weights=wtdper, # use survey weights (not relevant to qr)
              tau = mytau[t]) # tau ranges from 0 to 1
  coefs[t] <- myreg$coefficients[2]
  mysum <- summary(myreg)
  lb[t] <- coefs[t]-1.96*mysum$coefficients[2,2]
  ub[t] <- coefs[t]+1.96*mysum$coefficients[2,2]
}

# Construct a figure of coefficients across distribution
plotdata <- data.frame(mytau,coefs,lb,ub)
plotdata %>%ggplot(aes(x=mytau))+
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Activity Limitation on Health Spending")
```

Okay, this is a little bit hard to read. What does this look like if we ignore super spenders? 
```{r qr-plot-2}
plotdata %>% filter(mytau < 1) %>% ggplot(aes(x=mytau))+
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Activity Limitation on Health Spending")
```

We can also scale this based on the sample quantile of spending so that results are in percentages, rather than in levels (why might we want to do this?)

```{r qr-plot-3}
myquants <- quantile(mydata$exp_tot,probs=c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),na.rm=TRUE)
plotdata %>% mutate(coefs = coefs / myquants, 
                    lb = lb / myquants, 
                    ub = ub / myquants) %>% 
  ggplot(aes(x=mytau))+
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  geom_hline(yintercept=1,color='red',linetype='dashed') + 
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Activity Limitation on Health Spending")
# How do we interpret this figure? 
```

## Local polynomial Regression 

Suppose that instead of quantiles/deciles, we want a more flexible fit across the distribution. Let's estimate a nonparametric regression instead. Since we want to do something flexible, we'll need to be looking at two continuous variables, so let's shift from activity limitation to income as our explanatory variable of interest. 

### Binscatters

First, let's make some binscatter plots of teh relationship between income and spending. 

```{r lpr-1}
# Check this site for helpful coding tips for nonparametric techniques: https://nppackages.github.io/
# Binscatter: relationship between income and spending
mydata %>% ggplot(aes(x=lninc,y=exp_tot)) + 
  geom_point() + 
  theme_classic() + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  labs(x="Log(Income)",y="Total Health Expenditures") # This is hard to interpret

# Let's bin the data more
binsreg(y=exp_tot,x=lninc,
        w= ~ age + female + race_bl + eth_hisp + famsize + ed_col + ed_hsplus, # Any control variables we want
        data=mydata,
        line=c(3,3), # Do we want a smoothed line? 
        ci=c(3,3)) # If we want any confidence intervals on points
```

### Adding the regression

Okay, now let's weight the data and add a nonlinear regression to the plot: 

```{r lpr-2}
# Now we can add the local polynomial regression
# Note that this will add weights to data, which are all clustered
binsreg(y=exp_tot,x=lninc,
        w= ~ age + female + race_bl + eth_hisp + famsize + ed_col + ed_hsplus, # Any control variables we want
        data=mydata,
        polyreg=3,
        ci=c(3,3)) # If we want any confidence intervals on points

# Clean out NA observations
mydata <- mydata %>% filter(!is.na(mydata$lninc)) %>% filter(!is.na(mydata$exp_tot))
regdata <- mydata %>% sample_n(5000) # speed up estimation by sampling some of the data
m1 <- lprobust(y=regdata$exp_tot,x=regdata$lninc,
               neval = 30, # how many bins should we use
               p = 3, # max. polynomial order
               level=95, # confidence interval desired
               kernel = 'epa') # note: don't pub anything for h and b, there are companion commands for optimal bandwidth selectors
summary(m1) # estimates across distribution of lninc
nprobust.plot(m1) # 
```

## Causal Machine Learning Approaches: A quick introduction 

Note: these code chunks are here as a quick introduction; there is a lot more you can (and should!) do with these techniques if you're going to really use them in a causal design. 

### Random Forests

Let's implement a quick random forest: 
```{r random-forests}
library(randomForest)
myforest <- randomForest(exp_tot ~ ., data=regdata) # note: we just use all variables we have
print(myforest) # 82% of total variation explained, that's not bad
# predict(myforest, regdata) -- add "ECHO=F" to the code chunk above to avoid printing this 
hist(treesize(myforest)) # shows the size of each tree in the forest -- lots of decisions!
varImpPlot(myforest,n.var=15)

# Now we have a great model for predicting expenditures based on x; what's causal about it? 
```

### Neural Networks 

And a quick neural network: 

```{r nn}
library(neuralnet)
regdata <- regdata %>% mutate(female = as.numeric(female),
                              race_bl = as.numeric(race_bl),
                              race_oth = as.numeric(race_oth),
                              eth_hisp = as.numeric(eth_hisp),
                              ed_hs = as.numeric(ed_hs),
                              ed_col = as.numeric(ed_col))
nn <- neuralnet(exp_tot ~ lninc + age + female + race_bl + race_oth + eth_hisp + famsize + ed_hs + ed_col + lninc, 
                data=regdata,
                hidden=3,
                act.fct = "logistic",
                linear.output = FALSE)
plot(nn)

# Prediction
mydata <- mydata %>% mutate(female = as.numeric(female),
                              race_bl = as.numeric(race_bl),
                              race_oth = as.numeric(race_oth),
                              eth_hisp = as.numeric(eth_hisp),
                              ed_hs = as.numeric(ed_hs),
                              ed_col = as.numeric(ed_col))
mypreds <- compute(nn,mydata)
# mypreds$net.result*(max(mydata$exp_tot)-min(mydata$exp_tot))+min(mydata$exp_tot)
  # Again, add "echo=F" to the code chunk above before commenting this in. 
```

## Package Citations
```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS. 
```
