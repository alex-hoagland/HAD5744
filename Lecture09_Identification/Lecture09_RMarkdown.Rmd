---
title: "Lectures 7 and 8 Code"
author: "Alex Hoagland"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preliminaries}
################################################################################
# I like to include several additional notes in the header of my files here:
#
# Last modified: 8/12/2022
#
### PURPOSE:
  # Lectures 7 and 8 code and output file
# 
### NOTES:
  # - uses the Tidyverse package and Dplyr
################################################################################


### 0. Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom)
library(readxl) # Read in data
library(modelsummary) # For making regression tables
library(causaldata) # Useful toy data sets
library(here) # Helpful in working with directories and projects
library(AER) # this package has lots of applied metrics packages
library(foreign) # Helpful for reading in data from Stata or other code languages 
library(devtools)
# install_github("vincentarelbundock/marginaleffects")
library(marginaleffects) # To calculate marginal effects
library(pscl) # For hurdle models
library(nnet) # For multinomial logit models
library(knitr) # Alternative table package
library(kableExtra) # Alternative table package
library(gtsummary) # Alternative table package
library(fixest) 

set.seed(03262020) 
  # Setting the seed helps to make random number generators give us the same numbers across machines
################################################################################

mydata <- readxl::read_xlsx(here("HealthExpenses.xlsx"))
```

## Poisson Regression

Now, suppose that our outcome of interest is not whether or not an individual purchased insurance, but the number of health visits they selected in a year.

### Preliminaries

First, I will simulate some data and bake in some correlations. You can mostly ignore this code

```{r sim-data}
mydata$numvisits <- rpois(1340,2) # Base numvisits
# Now bake in some correlation
mydata <- mydata %>% 
  mutate(numvisits = ifelse(age >= 65,round(numvisits*1.5),numvisits)) %>%
  mutate(numvisits = ifelse(age < 35,round(numvisits*0.5),numvisits)) %>%
  mutate(numvisits = ifelse(sex == "male",round(numvisits*0.8),numvisits)) %>%
  mutate(numvisits = numvisits * bmi/31) %>%
  mutate(numvisits = ifelse(numvisits > 10, 10, numvisits)) %>% 
  mutate(numvisits = ifelse(smoker == 1, numvisits * 2, numvisits)) %>% 
  mutate(numvisits = numvisits * expenses/9000) %>% 
  mutate(numvisits = ifelse(numvisits > 10, 10, numvisits)) %>% 
  mutate(numvisits = ifelse(coverage == 1,round(numvisits*1.2),numvisits))
mydata$numvisits <- as.integer(mydata$numvisits)
hist(mydata$numvisits)
```

You can see that after the simulations, the distribution of the number of visits looks pretty similar to how it would look if we could use claims data.

### Poisson Regression

Let's use all of our variables to model the decision to choose health visits. In particular, suppose that we are after the impact of insurance coverage on total utilization (measured in visit counts)?

```{r poisson}
m_poisson <- glm(numvisits ~ coverage + age + sex + bmi + children + smoker,data=mydata,
                 family = "poisson"(link = "log")) # Why is our link function the log function?
m_naive <- lm(numvisits ~ coverage + age + sex + bmi + children + smoker,data=mydata) # Naive OLS
msummary(list("Naive"=m_naive,"Poisson"=m_poisson),
         vcov=c(rep("robust",2)),
         stars=c('*' = .1, '**' = .05, '***' = .01))
```

We interpret these regressions, again, by looking at the marginal effects (in this case, we can also look at rate ratios; the preference really depends on the field/journal more than the research question).

```{r me-p}
exp(m_poisson$coefficients)-1 # What is the marginal effect for coverage? What is the rate ratio?
```

### Testing for Dispersion

We also need to test for over-dispersion in order to run a Poisson model. That is, we want to test whether the mean and variance of our sample data are equivalent.

```{r dispersion}
var(mydata$numvisits)/mean(mydata$numvisits) # Lots of overdisperson! Could bootstrap this to get a CI. 
```

How does that affect our estimation decision? What can we do if we suffer from over-dispersion?

## Hurdle Model

Given that we have so much overdispersion, the Poisson model may not be the best choice -- we could look at a zero-inflated model or a negative binomial approach. But let's also consider the fact that many individuals may not use **any** care in a year, so that `numvisits=0`. Does our Poisson model capture that well?

```{r pred-zeros}
mydata$predicted_visits <- predict(m_poisson, type = "response")
ggplot(mydata)+
  geom_histogram(aes(x=numvisits),position='dodge',fill='blue',alpha=0.3)+
  geom_histogram(aes(x=predicted_visits),position='dodge',fill='red',alpha=0.5)+
  theme_classic()+
  labs(x="# of Visits",y="Count")

# sum the probabilities of a 0 count for each mean
exp <- sum(dpois(x = 0, lambda = mydata$predicted_visits))
round(exp) # predicted number of 0's
sum(mydata$numvisits == 0) # Observed number of 0's
```

Not really.

Instead, what if we estimated a hurdle model where we modeled the decision to seek **any** care (extensive margin) separately from the decision of how many visits to have (intensive margin). Note that when estimating a hurdle model, we have some package conflict issues with `modelsummary()` so our tables won't look quite as nice here.

```{r hurdle}
mod.hurdle <- hurdle(numvisits ~ age + sex + bmi + children + smoker + coverage, data = mydata)
  # First, suppose we have same covariates in both stage of the model 

# Model summary doesn't work as well with the pscl package
summary(mod.hurdle)$coefficients

# How does it do for prediction? 
sum(predict(mod.hurdle, type = "prob")[,1]) # Note: by design, hurdle model gives you exact number of 0s in data
mydata$new_pred <- floor(predict(mod.hurdle, type="response")) # Not sure that this prediction method is correct.
ggplot(mydata)+
  geom_histogram(aes(x=numvisits),position='dodge',fill='blue',alpha=0.3)+
  geom_histogram(aes(x=new_pred),position='dodge',fill='red',alpha=0.5)+
  theme_classic()+
  labs(x="# of Visits",y="Count")
```

## Multinomial Logit Regression

Finally, let's assume now that everyone in our data set has to have an MD visit, and either chooses (0) no treatment,(1) medication, or (2) surgery. How can we model the choice between these options? Multinomial logit!

### Preliminaries

Some more simulation of data:

```{r simulation-2}
mydata$choice <- sample(c(0,1,2),size=1340,replace=T,prob=c(0.5,0.25,0.25)) # Base choices
# Now bake in some correlation
mydata <- mydata %>% 
  mutate(choice = ifelse(age >= 65,round(choice*1.5),choice)) %>%
  mutate(choice = ifelse(choice > 2, 2, choice)) %>%
  mutate(choice = ifelse(age < 35,round(choice*0.5),choice)) %>%
  mutate(choice = ifelse(choice < 0, 0, choice)) %>%
  mutate(choice = ifelse(sex == "male",round(choice*1.5),choice)) %>%
  mutate(choice = ifelse(choice > 2, 2, choice)) %>%
  mutate(choice = ifelse(bmi >= 32 & choice == 2,sample(c(0,1),replace=T,prob=c(0.3,0.7)), choice)) %>%
  mutate(choice = ifelse(smoker == 1 & choice == 2,sample(c(0,1),replace=T,prob=c(0.5,0.5)), choice)) %>%
  mutate(choice = ifelse(smoker == 1 & choice == 1,sample(c(0,1),replace=T,prob=c(0.7,0.3)), choice)) %>%
  mutate(choice = ifelse(expenses >= 17000,round(choice*1.5),choice)) %>%
  mutate(choice = ifelse(choice > 2, 2, choice)) %>%
  mutate(choice = ifelse(coverage == 1,round(choice*1.5),choice)) %>%
  mutate(choice = ifelse(choice > 2, 2, choice))
hist(mydata$choice)
```

This shows us the choices individuals make in our (made up) data set. Now let's estimate!

### Estimation

First, we select our comparison outcome (in this case, no treatment). The best way to characterize this variable in R (for our package to understand it) is as a **factor**, which includes both a number ID and a value label for each choice in the discrete set. We've already been working with these a little bit (as dummy variables), but now we want to really have a 3-plus level factor variable for our multinomial logit to make sense.

Again, `modelsummary()` won't work well here, so we'll use a separate package to make some tables.

```{r mn-logit}
mydata$choice <- factor(mydata$choice,levels=c(0,1,2),labels=c("Monitoring","Medication","Surgery")) 

mydata$region <- factor(mydata$region,levels=c("northeast","northwest","southeast","southwest"),labels=c("northeast","northwest","southeast","southwest"))
  # Let's us include this straight and calculate dummies automatically

mydata$choice_relative <- relevel(mydata$choice, ref = "Monitoring")
mnl <- multinom(choice_relative ~ age + sex + bmi + children + smoker + expenses + coverage + region,
                data = mydata) # Discuss iterations and output

summary(mnl)
exp(coef(mnl)) # Need to exponentiate to get odds 
```

### Presentation

Now that the model has been estimated, let's try (being the operative word) to present this in a well-formatted and easy-to-understand way. This model is already complicated, so even without the package conflicts it can be tricky to know how to summarize it. I suggest a few things:

-   A regression table showing the marginal effects + standard errors for each of the $J-1$ sets of $K$ coefficients.

-   

```{r mnl-presentation}
# Store coefficients as relative changes in probability
relprobs <- exp(coef(mnl)) # These are the relative probability changes for each outcome
relprobs <- data.frame(t(relprobs)) 
relprobs <- relprobs %>%
  mutate(variable = row.names(relprobs)) %>%
  pivot_longer(cols=c("Medication","Surgery"),names_to = "group") 
  # Convert this to one column

# Add in standard errors for coefficients
relses <- summary(mnl)$standard.errors
relses <- data.frame(t(relses)) 
relses <- relses %>%
  mutate(variable = row.names(relses)) %>%
  pivot_longer(cols=c("Medication","Surgery"),names_to = "group",values_to="std.error") 
  # Convert this to one column

# combine
table_mnl <- full_join(relprobs,relses,by=c("group","variable"))

# Need to rename some rows so that the marginal effects merge in 
table_mnl <- table_mnl %>% mutate(variable = ifelse(variable=="sexmale","sex",variable))

# Calculate marginal effects: average marginal effect
myme <- marginaleffects(mnl, type = "probs")
ame <- myme %>% group_by(group, contrast, term) %>% summarize(dydx = mean(dydx),
                                                    stderr = median(std.error),
                                                    p = median(p.value))
# Rename some rows to merge correctly 
ame <- ame %>% mutate(term = ifelse(term == "region" & contrast == "northwest - northeast",
                                    "regionnorthwest",
                                    ifelse(term == "region" & contrast == "southwest - northeast",
                                           "regionsouthwest",
                                           ifelse(term=="region" & contrast == "southeast - northeast",
                                                  "regionsoutheast",term)))) %>%
  ungroup() %>% select(-c(contrast))

names(ame) <- c("group","variable","ame","ame_stderr","ame_p")

# Constructing a manual table with Kable extra: 
table_mnl <- full_join(table_mnl,ame,by=c("group","variable"),)
table_mnl$group <- factor(table_mnl$group,levels=c("Medication","Surgery","Monitoring"))
table_mnl$variable <- factor(table_mnl$variable,
                             levels=c("(Intercept)","age","sex","bmi","children","smoker",
                                      "expenses","coverage","regionnorthwest","regionsoutheast",
                                      "regionsouthwest"),
                             labels=c("Level Shift","Age","Male","BMI","# of Children","Smoker",
                                      "Total Health Expenses",
                                      "Insurance Coverage","Region: Northwest","Region: Southeast",
                                      "Region: Southwest"))
table_mnl <- table_mnl %>% arrange(group) %>% 
  select(group,variable,value,std.error,ame,ame_stderr,ame_p) # sort the table appropriately

# Format it with kable extra
options(knitr.kable.NA = '') # Gives us blanks instead of NAs in the table
table_mnl %>% select(-c(group)) %>% # Remove name of group
  kableExtra::kable(format = "html", digits=3,
  col.names = c("Regressor","Multiplicative Effect", "SE", "Average", "SE", "p-value"),escape = F) %>%
  kableExtra::add_header_above(c(" " = 1, "Exp(Regression Coefficients)" = 2, "Marginal Effects" = 3)) %>%
  kableExtra::group_rows("Panel A: Medication", 1, 11) %>%
  kableExtra::group_rows("Panel B: Surgery", 12, 22) %>%
  kableExtra::group_rows("Panel C: Monitoring", 23, 32) %>%
  kableExtra::kable_styling(bootstrap_options = c("hover"),full_width = F,font_size=12)
```

### Model Performance

Finally, we need to check the accuracy of the model. Let's do this in a table *and* a figure.

```{r accuracy}
mydata$predicted_choice <- predict(mnl,newdata=mydata,"class")
ctable <- table(mydata$choice_relative,mydata$predicted_choice) # Building classification table
ctable
round((sum(diag(ctable))/sum(ctable))*100,2) # Calculating accuracy - sum of diagonal elements divided by total obs
  # 71.12% accuracy doesn't sound bad. 

ggplot(data=mydata,aes()) + 
  geom_histogram(aes(x=as.numeric(choice)-.1),fill='blue',alpha=0.3,position='dodge') + 
  geom_histogram(aes(x=as.numeric(predicted_choice)+.1),fill='red',alpha=0.5,position='dodge') + 
  theme_minimal() + 
  labs(x="Choice",y="Count") # But don't forget to visualize it! 
```

## Package Citations

```{r, include=FALSE}
print("=============================Works Cited=============================")
loadedNamespaces() %>%
map(citation) %>%
print(style = "text") # Adds citations for each package to end of .rmd file

knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this lecture.

# DON'T FORGET TO CITE YOUR PACKAGES IN YOUR PAPERS/ASSIGNMENTS.  
```
